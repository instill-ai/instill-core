{{ template "chart.header" . }}

{{ template "chart.deprecationWarning" . }}

{{ template "chart.versionBadge" . }}{{ template "chart.typeBadge" . }}{{ template "chart.appVersionBadge" . }}

{{ template "chart.description" . }}

- [Introduction](#introduction)
- [Deployment](#deployment)
  - [KIND cluster configuration](#kind-cluster-configuration)
  - [Model repository](#model-repository)
    - [Locally accessible filesystem](#locally-accessible-filesystem)
  - [Installing the Chart](#installing-the-chart)
    - [Pull images from a private registry](#pull-images-from-a-private-registry)
    - [Deploy Triton inference server](#deploy-triton-inference-server)
      - [Install from source](#install-from-source)
      - [Install from repository](#install-from-repository)
  - [Use the inference server](#use-the-inference-server)
  - [Uninstalling the Chart](#uninstalling-the-chart)
- [Values](#values)

## Introduction
This chart installs a [NVIDIA Triton
Inference Server](https://github.com/NVIDIA/triton-inference-server) on a Kubernetes cluster using Helm package manager. By default the cluster contains a single instance of the inference server but the *replicaCount* configuration parameter can be set to create a cluster of any size, as described below. This guide assumes you already have a functional Kubernetes
cluster and helm installed. If you want Triton Server to use GPUs for inferencing, your cluster must be configured with support for the NVIDIA driver and CUDA version required by the version of the inference server you are using.

The steps below describe how to use helm to launch the inference server.

## Deployment
The steps below describe how to use helm to launch the inference server and then send inference requests to the running server.
>:information_source: Read [KIND cluster configuration](#kind-cluster-configuration) if you are deploying on a kinD cluster.

### KIND cluster configuration
If you are deploying on a kinD cluster, you need to configure the kinD cluster creation:
 - Extra mounts: mount the model repository from host machine to the kinD cluster with `extraMount`
 - Extra port mapping to expose a `NodePort` service: map ports to the host machine with `extraPortMappings` to get traffic into the kinD cluster

Here, we provide a yaml config file as an example:
```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraMounts:
  - hostPath: [model_repository_on_host]
    containerPath: [model_repository_on_kind]
  extraPortMappings:
  - containerPort: [http_nodeport]
    hostPort: [http_nodeport]
    protocol: TCP
  - containerPort: [grpc_nodeport]
    hostPort: [grpc_nodeport]
    protocol: TCP
  - containerPort: [metrics_nodePort]
    hostPort: [metrics_nodePort]
    protocol: TCP
```
where
- `model_repository_on_host` points to the model directory that stores models on the host machine
- `model_repository_on_kind` is the value of `image.modelRepositoryPath` in `single_server/values.yaml`, kinD mounts `model_repository_on_host` from host machine to `model_repository_on_kind` on the kinD cluster
- `http_nodeport` is the nodeport of the HTTP endpoint
- `grpc_nodeport` is the nodeport of the gRPC endpoint
- `metrics_nodeport` is the nodeport of the inference server metrics endpoint

### Model repository
Triton inference server needs a repository of models that it will make available for inferencing. The repository path is specified when Triton is started using `--model-store` or `--model-repository` option.

You can place the model repository in
- [**for now**] local accessible filesystem: models are stored under `/models` by default
- Google Cloud Storage
- Amazon S3

#### Locally accessible filesystem
For now, we use a [`hostPath`](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath) volume to mount the directory from host nodes's filesystem into `/models` of the container. Config `image.modelRepositoryPath` in `single_server/values.yaml` to point to the local directory on the host node.

>:information_source: `hostPath` volume is **NOT** recommended and will be improved in the future:
> - The Pod is tightly coupled to the underlying volume and the node. It is against the Kubernetes principle: workload portability.
> - The server is run as non-root user (user `1000` from group `1000`). The files or directories created on the underlying hosts are only writable by root. You either need to run your process as root in a privileged Container or modify the file permissions on the host to be able to write to a `hostPath` volume.

### Installing the Chart

##### Install from source
To install the chart with the release name `example`:
```bash
$ helm dependency update  # retrieve dependent charts into charts/ directory
$ helm install example .  # deploy the server
```

### Uninstalling the Chart
To uninstall/delete the `example` deployment and the `example-metrics` deployment if enabled:
```bash
$ helm uninstall example # delete the inference server
```
The command removes all the Kubernetes components associated with the chart and deletes the release.

## Values

Values for the Triton Server are described below.

{{ template "chart.valuesTable" . }}
